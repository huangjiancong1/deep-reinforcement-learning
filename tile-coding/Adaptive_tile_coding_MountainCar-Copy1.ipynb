{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import common libraries\n",
    "import sys\n",
    "import gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# Set plotting options\n",
    "%matplotlib inline\n",
    "plt.style.use('ggplot')\n",
    "np.set_printoptions(precision=3, linewidth=120)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "State space: Box(2,)\n",
      "- low: [-1.2  -0.07]\n",
      "- high: [0.6  0.07]\n",
      "Action space: Discrete(3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jim/gym/gym/envs/registration.py:14: PkgResourcesDeprecationWarning: Parameters to load are deprecated.  Call .resolve and .require separately.\n",
      "  result = entry_point.load(False)\n"
     ]
    }
   ],
   "source": [
    "# Create an environment\n",
    "env = gym.make('MountainCar-v0')\n",
    "env.seed(505);\n",
    "\n",
    "# Explore state (observation) space\n",
    "print(\"State space:\", env.observation_space)\n",
    "print(\"- low:\", env.observation_space.low)\n",
    "print(\"- high:\", env.observation_space.high)\n",
    "\n",
    "# Explore action space\n",
    "print(\"Action space:\", env.action_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_tiling_grid(low, high, bins=(10, 10), offsets=(0.0, 0.0)):\n",
    "    \"\"\"Define a uniformly-spaced grid that can be used for tile-coding a space.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    low : array_like\n",
    "        Lower bounds for each dimension of the continuous space.\n",
    "    high : array_like\n",
    "        Upper bounds for each dimension of the continuous space.\n",
    "    bins : tuple\n",
    "        Number of bins or tiles along each corresponding dimension.\n",
    "    offsets : tuple\n",
    "        Split points for each dimension should be offset by these values.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    grid : list of array_like\n",
    "        A list of arrays containing split points for each dimension.\n",
    "    \"\"\"\n",
    "    # TODO: Implement this\n",
    "    grid = [np.linspace(low[dim], high[dim], bins[dim] + 1)[1:-1] + offsets[dim] for dim in range(len(bins))]\n",
    "    print(\"Tiling: [<low>, <high>] / <bins> + (<offset>) => <splits>\")\n",
    "    for l, h, b, o, splits in zip(low, high, bins, offsets, grid):\n",
    "        print(\"    [{}, {}] / {} + ({}) => {}\".format(l, h, b, o, splits))\n",
    "    return grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discretize(sample, grid):\n",
    "    \"\"\"Discretize a sample as per given grid.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    sample : array_like\n",
    "        A single sample from the (original) continuous space.\n",
    "    grid : list of array_like\n",
    "        A list of arrays containing split points for each dimension.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    discretized_sample : array_like\n",
    "        A sequence of integers with the same number of dimensions as sample.\n",
    "    \"\"\"\n",
    "    # TODO: Implement this\n",
    "    return tuple(int(np.digitize(s, g)) for s, g in zip(sample, grid))  # apply along each dimension\n",
    "#     return list(int(np.digitize(s, g)) for s, g in zip(sample, grid))  # apply along each dimension\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tiling: [<low>, <high>] / <bins> + (<offset>) => <splits>\n",
      "    [-1.0, 1.0] / 10 + (0.0) => [-0.8 -0.6 -0.4 -0.2  0.   0.2  0.4  0.6  0.8]\n",
      "    [-5.0, 5.0] / 10 + (0.0) => [-4. -3. -2. -1.  0.  1.  2.  3.  4.]\n",
      "\n",
      "Samples:\n",
      "array([[-1.  , -5.  ],\n",
      "       [-0.81, -4.1 ],\n",
      "       [-0.8 , -4.  ],\n",
      "       [-0.5 ,  0.  ],\n",
      "       [ 0.2 , -1.9 ],\n",
      "       [ 0.8 ,  4.  ],\n",
      "       [ 0.81,  4.1 ],\n",
      "       [ 1.  ,  5.  ]])\n",
      "\n",
      "Discretized samples:\n",
      "array([[0, 0],\n",
      "       [0, 0],\n",
      "       [1, 1],\n",
      "       [2, 5],\n",
      "       [5, 3],\n",
      "       [9, 9],\n",
      "       [9, 9],\n",
      "       [9, 9]])\n",
      "<class 'list'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[array([-0.8, -0.6, -0.4, -0.2,  0. ,  0.2,  0.4,  0.6,  0.8]),\n",
       " array([-4., -3., -2., -1.,  0.,  1.,  2.,  3.,  4.])]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid = create_tiling_grid([-1.0, -5.0], [1.0, 5.0])\n",
    "samples = np.array(\n",
    "    [[-1.0 , -5.0],\n",
    "     [-0.81, -4.1],\n",
    "     [-0.8 , -4.0],\n",
    "     [-0.5 ,  0.0],\n",
    "     [ 0.2 , -1.9],\n",
    "     [ 0.8 ,  4.0],\n",
    "     [ 0.81,  4.1],\n",
    "     [ 1.0 ,  5.0]])\n",
    "discretized_samples = np.array([discretize(sample, grid) for sample in samples])\n",
    "print(\"\\nSamples:\", repr(samples), sep=\"\\n\")\n",
    "print(\"\\nDiscretized samples:\", repr(discretized_samples), sep=\"\\n\")\n",
    "print(type(grid))\n",
    "grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_tilings(low, high, tiling_specs):\n",
    "    \"\"\"Define multiple tilings using the provided specifications.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    low : array_like\n",
    "        Lower bounds for each dimension of the continuous space.\n",
    "    high : array_like\n",
    "        Upper bounds for each dimension of the continuous space.\n",
    "    tiling_specs : list of tuples\n",
    "        A sequence of (bins, offsets) to be passed to create_tiling_grid().\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    tilings : list\n",
    "        A list of tilings (grids), each produced by create_tiling_grid().\n",
    "    \"\"\"\n",
    "    # TODO: Implement this\n",
    "    return [create_tiling_grid(low, high, bins, offsets) for bins, offsets in tiling_specs]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.lines import Line2D\n",
    "\n",
    "# import pdb\n",
    "\n",
    "def visualize_tilings(tilings):\n",
    "    \"\"\"Plot each tiling as a grid.\"\"\"\n",
    "#     pdb.set_trace()\n",
    "    prop_cycle = plt.rcParams['axes.prop_cycle']\n",
    "    colors = prop_cycle.by_key()['color']\n",
    "    linestyles = ['-', '--', ':']\n",
    "    legend_lines = []\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(10, 10))\n",
    "    for i, grid in enumerate(tilings):\n",
    "        for x in grid[0]:\n",
    "            l = ax.axvline(x=x, color=colors[i % len(colors)], linestyle=linestyles[i % len(linestyles)], label=i)\n",
    "        for y in grid[1]:\n",
    "            l = ax.axhline(y=y, color=colors[i % len(colors)], linestyle=linestyles[i % len(linestyles)])\n",
    "        legend_lines.append(l)\n",
    "    ax.grid('off')\n",
    "    ax.legend(legend_lines, [\"Tiling #{}\".format(t) for t in range(len(legend_lines))], facecolor='white', framealpha=0.9)\n",
    "    ax.set_title(\"Tilings\")\n",
    "    return ax  # return Axis object to draw on later, if needed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.collections as mc\n",
    "\n",
    "def visualize_samples(samples, discretized_samples, grid, low=None, high=None):\n",
    "    \"\"\"Visualize original and discretized samples on a given 2-dimensional grid.\"\"\"\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(10, 10))\n",
    "    \n",
    "    # Show grid\n",
    "    ax.xaxis.set_major_locator(plt.FixedLocator(grid[0]))\n",
    "    ax.yaxis.set_major_locator(plt.FixedLocator(grid[1]))\n",
    "    ax.grid(True)\n",
    "    \n",
    "    # If bounds (low, high) are specified, use them to set axis limits\n",
    "    if low is not None and high is not None:\n",
    "        ax.set_xlim(low[0], high[0])\n",
    "        ax.set_ylim(low[1], high[1])\n",
    "    else:\n",
    "        # Otherwise use first, last grid locations as low, high (for further mapping discretized samples)\n",
    "        low = [splits[0] for splits in grid]\n",
    "        high = [splits[-1] for splits in grid]\n",
    "\n",
    "    # Map each discretized sample (which is really an index) to the center of corresponding grid cell\n",
    "    grid_extended = np.hstack((np.array([low]).T, grid, np.array([high]).T))  # add low and high ends\n",
    "    grid_centers = (grid_extended[:, 1:] + grid_extended[:, :-1]) / 2  # compute center of each grid cell\n",
    "    locs = np.stack(grid_centers[i, discretized_samples[:, i]] for i in range(len(grid))).T  # map discretized samples\n",
    "\n",
    "    ax.plot(samples[:, 0], samples[:, 1], 'o')  # plot original samples\n",
    "    ax.plot(locs[:, 0], locs[:, 1], 's')  # plot discretized samples in mapped locations\n",
    "    ax.add_collection(mc.LineCollection(list(zip(samples, locs)), colors='orange'))  # add a line connecting each original-discretized sample\n",
    "    ax.legend(['original', 'discretized'])\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tile_encode(sample, tilings, flatten=False):\n",
    "    \"\"\"Encode given sample using tile-coding.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    sample : array_like\n",
    "        A single sample from the (original) continuous space.\n",
    "    tilings : list\n",
    "        A list of tilings (grids), each produced by create_tiling_grid().\n",
    "    flatten : bool\n",
    "        If true, flatten the resulting binary arrays into a single long vector.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    encoded_sample : list or array_like\n",
    "        A list of binary vectors, one for each tiling, or flattened into one.\n",
    "    \"\"\"\n",
    "    # TODO: Implement this\n",
    "#     encoded_sample = [discretize(sample, grid) for grid in tilings]\n",
    "#     encoded_sample = discretize(sample, tilings)\n",
    "    encoded_sample = list(discretize(sample, tilings))\n",
    "    return np.concatenate(encoded_sample) if flatten else encoded_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pdb\n",
    "class QTable:\n",
    "    \"\"\"Simple Q-table.\"\"\"\n",
    "\n",
    "    def __init__(self, state_size, action_size):\n",
    "        \"\"\"Initialize Q-table.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        state_size : tuple\n",
    "            Number of discrete values along each dimension of state space.\n",
    "        action_size : int\n",
    "            Number of discrete actions in action space.\n",
    "        \"\"\"\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "\n",
    "        # TODO: Create Q-table, initialize all Q-values to zero\n",
    "        # Note: If state_size = (9, 9), action_size = 2, q_table.shape should be (9, 9, 2)\n",
    "        self.q_table = np.zeros(shape=(self.state_size + (self.action_size,)))\n",
    "        print(\"QTable(): size =\", self.q_table.shape)\n",
    "\n",
    "\n",
    "class TiledQTable:\n",
    "    \"\"\"Composite Q-table with an internal tile coding scheme.\"\"\"\n",
    "    \n",
    "    def __init__(self, low, high, tiling_specs, action_size):\n",
    "        \"\"\"Create tilings and initialize internal Q-table(s).\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        low : array_like\n",
    "            Lower bounds for each dimension of state space.\n",
    "        high : array_like\n",
    "            Upper bounds for each dimension of state space.\n",
    "        tiling_specs : list of tuples\n",
    "            A sequence of (bins, offsets) to be passed to create_tiling_grid() along with low, high.\n",
    "        action_size : int\n",
    "            Number of discrete actions in action space.\n",
    "        \"\"\"\n",
    "#         pdb.set_trace()\n",
    "\n",
    "#         self.tilings = create_tilings(low, high, tiling_specs)\n",
    "        self.tilings = create_tiling_grid(low, high)\n",
    "#         self.state_sizes = [tuple(len(splits)+1 for splits in tiling_grid) for tiling_grid in self.tilings]\n",
    "        self.state_sizes = tuple(len(splits) + 1 for splits in self.tilings)\n",
    "        self.action_size = action_size\n",
    "#         self.q_tables = [QTable(state_size, self.action_size) for state_size in self.state_sizes]\n",
    "        self.q_tables = QTable(self.state_sizes, self.action_size)\n",
    "#         self.q_tables = np.zeros(shape=(self.state_sizes + (self.action_size,)))\n",
    "#         self.q_tables = np.zeros(shape=(self.state_sizes + (self.action_size,))).tolist()\n",
    "#         print(\"TiledQTable(): no. of internal tables = \", len(self.q_tables))\n",
    "    \n",
    "    def get(self, state, action):\n",
    "        \"\"\"Get Q-value for given <state, action> pair.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        state : array_like\n",
    "            Vector representing the state in the original continuous space.\n",
    "        action : int\n",
    "            Index of desired action.\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        value : float\n",
    "            Q-value of given <state, action> pair, averaged from all internal Q-tables.\n",
    "        \"\"\"\n",
    "        # TODO: Encode state to get tile indices\n",
    "#         pdb.set_trace()\n",
    "        encoded_state = tile_encode(state, self.tilings)\n",
    "#         encoded_state = np.array([discretize(sample, self.tilings) for sample in state])\n",
    "        # TODO: Retrieve q-value for each tiling, and return their average\n",
    "        value = 0.0\n",
    "        for idx, q_table in zip(encoded_state, self.q_tables):\n",
    "#             value += q_table.q_table[tuple(idx + (action,))]\n",
    "            value += q_table[tuple(idx + (action,))]\n",
    "#             value += q_table[idx + (action,)]\n",
    "\n",
    "# (tuple(encoded_state) + (action,))\n",
    "\n",
    "        value /= len(self.q_tables)\n",
    "        return value\n",
    "    \n",
    "    def update(self, state, action, value, alpha=0.1):\n",
    "        \"\"\"Soft-update Q-value for given <state, action> pair to value.\n",
    "        \n",
    "        Instead of overwriting Q(state, action) with value, perform soft-update:\n",
    "            Q(state, action) = alpha * value + (1.0 - alpha) * Q(state, action)\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        state : array_like\n",
    "            Vector representing the state in the original continuous space.\n",
    "        action : int\n",
    "            Index of desired action.\n",
    "        value : float\n",
    "            Desired Q-value for <state, action> pair.\n",
    "        alpha : float\n",
    "            Update factor to perform soft-update, in [0.0, 1.0] range.\n",
    "        \"\"\"\n",
    "        # TODO: Encode state to get tile indices\n",
    "        encoded_state = tile_encode(state, self.tilings)\n",
    "        \n",
    "        # TODO: Update q-value for each tiling by update factor alpha\n",
    "        for idx, q_table in zip(encoded_state, self.q_tables):\n",
    "#             value_ = q_table.q_table[tuple(idx + (action,))]  # current value\n",
    "            value_ = q_table[tuple(idx + (action,))]  # current value\n",
    "            q_table.q_table[tuple(idx + (action,))] = alpha * value + (1.0 - alpha) * value_\n",
    "\n",
    "\n",
    "\n",
    "# # Test with a sample Q-table\n",
    "# tq = TiledQTable(low, high, tiling_specs, 2)\n",
    "# s1 = 3; s2 = 4; a = 0; q = 1.0\n",
    "# print(\"[GET]    Q({}, {}) = {}\".format(samples[s1], a, tq.get(samples[s1], a)))  # check value at sample = s1, action = a\n",
    "# print(\"[UPDATE] Q({}, {}) = {}\".format(samples[s2], a, q)); tq.update(samples[s2], a, q)  # update value for sample with some common tile(s)\n",
    "# print(\"[GET]    Q({}, {}) = {}\".format(samples[s1], a, tq.get(samples[s1], a)))  # check value again, should be slightly updated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pdb\n",
    "class QLearningAgent:\n",
    "    \"\"\"Q-Learning agent that can act on a continuous state space by discretizing it.\"\"\"\n",
    "\n",
    "    def __init__(self, env, tq, alpha=0.02, gamma=0.99,\n",
    "                 epsilon=1.0, epsilon_decay_rate=0.9995, min_epsilon=.01, seed=0):\n",
    "        \"\"\"Initialize variables, create grid for discretization.\"\"\"\n",
    "        # Environment info\n",
    "        self.env = env\n",
    "        self.tq = tq \n",
    "        self.q_table = tq.q_tables\n",
    "        self.state_sizes = tq.state_sizes           # list of state sizes for each tiling\n",
    "        self.action_size = self.env.action_space.n  # 1-dimensional discrete action space\n",
    "        self.seed = np.random.seed(seed)\n",
    "        print(\"Environment:\", self.env)\n",
    "        print(\"State space sizes:\", self.state_sizes)\n",
    "        print(\"Action space size:\", self.action_size)\n",
    "        \n",
    "        # Learning parameters\n",
    "        self.alpha = alpha  # learning rate\n",
    "        self.gamma = gamma  # discount factor\n",
    "        self.epsilon = self.initial_epsilon = epsilon  # initial exploration rate\n",
    "        self.epsilon_decay_rate = epsilon_decay_rate   # how quickly should we decrease epsilon\n",
    "        self.min_epsilon = min_epsilon\n",
    "\n",
    "    def reset_episode(self, state):\n",
    "        \"\"\"Reset variables for a new episode.\"\"\"\n",
    "        # Gradually decrease exploration rate\n",
    "        self.epsilon *= self.epsilon_decay_rate\n",
    "        self.epsilon = max(self.epsilon, self.min_epsilon)\n",
    "        \n",
    "        self.last_state = state\n",
    "        Q_s = [self.tq.get(state, action) for action in range(self.action_size)]\n",
    "        self.last_action = np.argmax(Q_s)\n",
    "        return self.last_action\n",
    "    \n",
    "    def reset_exploration(self, epsilon=None):\n",
    "        \"\"\"Reset exploration rate used when training.\"\"\"\n",
    "        self.epsilon = epsilon if epsilon is not None else self.initial_epsilon\n",
    "\n",
    "    def act(self, state, reward=None, done=None, mode='train'):\n",
    "        \"\"\"Pick next action and update internal Q table (when mode != 'test').\"\"\"\n",
    "        Q_s = [self.tq.get(state, action) for action in range(self.action_size)]\n",
    "        # Pick the best action from Q table\n",
    "        greedy_action = np.argmax(Q_s)\n",
    "        if mode == 'test':\n",
    "            # Test mode: Simply produce an action\n",
    "            action = greedy_action\n",
    "        else:\n",
    "            # Train mode (default): Update Q table, pick next action\n",
    "            # Note: We update the Q table entry for the *last* (state, action) pair with current state, reward\n",
    "            value = reward + self.gamma * max(Q_s)\n",
    "            self.tq.update(self.last_state, self.last_action, value, self.alpha)\n",
    "\n",
    "            # Exploration vs. exploitation\n",
    "            do_exploration = np.random.uniform(0, 1) < self.epsilon\n",
    "            if do_exploration:\n",
    "                # Pick a random action\n",
    "                action = np.random.randint(0, self.action_size)\n",
    "            else:\n",
    "                # Pick the greedy action\n",
    "                action = greedy_action\n",
    "\n",
    "        # Roll over current state, action for next step\n",
    "        self.last_state = state\n",
    "        self.last_action = action\n",
    "        return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tiling: [<low>, <high>] / <bins> + (<offset>) => <splits>\n",
      "    [-1.2000000476837158, 0.6000000238418579] / 10 + (0.0) => [-1.02 -0.84 -0.66 -0.48 -0.3  -0.12  0.06  0.24  0.42]\n",
      "    [-0.07000000029802322, 0.07000000029802322] / 10 + (0.0) => [-0.056 -0.042 -0.028 -0.014  0.     0.014  0.028  0.042  0.056]\n",
      "QTable(): size = (10, 10, 3)\n",
      "Environment: <TimeLimit<MountainCarEnv<MountainCar-v0>>>\n",
      "State space sizes: (10, 10)\n",
      "Action space size: 3\n"
     ]
    }
   ],
   "source": [
    "n_bins = 10\n",
    "bins = tuple([n_bins]*env.observation_space.shape[0])\n",
    "offset_pos = (env.observation_space.high - env.observation_space.low)/(3*n_bins)\n",
    "\n",
    "tiling_specs = [(bins, tuple([0.0]*env.observation_space.shape[0]))]\n",
    "\n",
    "tq = TiledQTable(env.observation_space.low, \n",
    "                 env.observation_space.high, \n",
    "                 tiling_specs, \n",
    "                 env.action_space.n)\n",
    "agent = QLearningAgent(env, tq)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.collections as mc\n",
    "\n",
    "def visualize_samples(samples, discretized_samples, grid, low=None, high=None):\n",
    "    \"\"\"Visualize original and discretized samples on a given 2-dimensional grid.\"\"\"\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(10, 10))\n",
    "    \n",
    "    # Show grid\n",
    "    ax.xaxis.set_major_locator(plt.FixedLocator(grid[0]))\n",
    "    ax.yaxis.set_major_locator(plt.FixedLocator(grid[1]))\n",
    "    ax.grid(True)\n",
    "    \n",
    "    # If bounds (low, high) are specified, use them to set axis limits\n",
    "    if low is not None and high is not None:\n",
    "        ax.set_xlim(low[0], high[0])\n",
    "        ax.set_ylim(low[1], high[1])\n",
    "    else:\n",
    "        # Otherwise use first, last grid locations as low, high (for further mapping discretized samples)\n",
    "        low = [splits[0] for splits in grid]\n",
    "        high = [splits[-1] for splits in grid]\n",
    "\n",
    "    # Map each discretized sample (which is really an index) to the center of corresponding grid cell\n",
    "    grid_extended = np.hstack((np.array([low]).T, grid, np.array([high]).T))  # add low and high ends\n",
    "    grid_centers = (grid_extended[:, 1:] + grid_extended[:, :-1]) / 2  # compute center of each grid cell\n",
    "    locs = np.stack(grid_centers[i, discretized_samples[:, i]] for i in range(len(grid))).T  # map discretized samples\n",
    "\n",
    "    ax.plot(samples[:, 0], samples[:, 1], 'o')  # plot original samples\n",
    "    ax.plot(locs[:, 0], locs[:, 1], 's')  # plot discretized samples in mapped locations\n",
    "    ax.add_collection(mc.LineCollection(list(zip(samples, locs)), colors='orange'))  # add a line connecting each original-discretized sample\n",
    "    ax.legend(['original', 'discretized'])\n",
    "\n",
    "    \n",
    "# state_samples = np.array([env.observation_space.sample() for i in range(10)])\n",
    "# discretized_state_samples = np.array([discretize(sample, tilings) for sample in state_samples])\n",
    "# visualize_samples(state_samples, discretized_state_samples, tilings, env.observation_space.low, env.observation_space.high)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tiling: [<low>, <high>] / <bins> + (<offset>) => <splits>\n",
      "    [-1.2000000476837158, 0.6000000238418579] / 10 + (0.0) => [-1.02 -0.84 -0.66 -0.48 -0.3  -0.12  0.06  0.24  0.42]\n",
      "    [-0.07000000029802322, 0.07000000029802322] / 10 + (0.0) => [-0.056 -0.042 -0.028 -0.014  0.     0.014  0.028  0.042  0.056]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# state_grid = tilings\n",
    "state_grid = create_tiling_grid(env.observation_space.low, env.observation_space.high, bins=(10, 10))\n",
    "# state_grid = TiledQTable.tilings\n",
    "\n",
    "\n",
    "# Obtain some samples from the space, discretize them, and then visualize them\n",
    "state_samples = np.array([env.observation_space.sample() for i in range(10)])\n",
    "discretized_state_samples = np.array([discretize(sample, state_grid) for sample in state_samples])\n",
    "visualize_samples(state_samples, discretized_state_samples, state_grid,\n",
    "                  env.observation_space.low, env.observation_space.high)\n",
    "plt.xlabel('position'); plt.ylabel('velocity');  # axis labels for MountainCar-v0 state space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "zip argument #2 must support iteration",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-32-d026a8a1f056>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mscores\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0magent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-32-d026a8a1f056>\u001b[0m in \u001b[0;36mrun\u001b[0;34m(agent, env, num_episodes, mode)\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0;31m# Initialize episode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m         \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_episode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m         \u001b[0mtotal_reward\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mdone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-25-bc399709d3a2>\u001b[0m in \u001b[0;36mreset_episode\u001b[0;34m(self, state)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlast_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m         \u001b[0mQ_s\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0maction\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlast_action\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mQ_s\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlast_action\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-25-bc399709d3a2>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlast_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m         \u001b[0mQ_s\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0maction\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlast_action\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mQ_s\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlast_action\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-24-71a23b3eb561>\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, state, action)\u001b[0m\n\u001b[1;32m     73\u001b[0m         \u001b[0;31m# TODO: Retrieve q-value for each tiling, and return their average\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m         \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mq_table\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoded_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mq_tables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     76\u001b[0m \u001b[0;31m#             value += q_table.q_table[tuple(idx + (action,))]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mq_table\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: zip argument #2 must support iteration"
     ]
    }
   ],
   "source": [
    "def run(agent, env, num_episodes=10000, mode='train'):\n",
    "    \"\"\"Run agent in given reinforcement learning environment and return scores.\"\"\"\n",
    "    scores = []\n",
    "    max_avg_score = -np.inf\n",
    "    for i_episode in range(1, num_episodes+1):\n",
    "        # Initialize episode\n",
    "        state = env.reset()\n",
    "        action = agent.reset_episode(state)\n",
    "        total_reward = 0\n",
    "        done = False\n",
    "\n",
    "        # Roll out steps until done\n",
    "        while not done:\n",
    "            state, reward, done, info = env.step(action)\n",
    "            total_reward += reward\n",
    "            action = agent.act(state, reward, done, mode)\n",
    "\n",
    "        # Save final score\n",
    "        scores.append(total_reward)\n",
    "\n",
    "        # Print episode stats\n",
    "        if mode == 'train':\n",
    "            if len(scores) > 100:\n",
    "                avg_score = np.mean(scores[-100:])\n",
    "                if avg_score > max_avg_score:\n",
    "                    max_avg_score = avg_score\n",
    "            if i_episode % 100 == 0:\n",
    "                print(\"\\rEpisode {}/{} | Max Average Score: {}\".format(i_episode, num_episodes, max_avg_score), end=\"\")\n",
    "                sys.stdout.flush()\n",
    "    return scores\n",
    "\n",
    "scores = run(agent, env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<__main__.QTable at 0x7f3afa5b4630>]"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.q_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "AxisError",
     "evalue": "axis 2 is out of bounds for array of dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAxisError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-73-6613428292fc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0mplot_q_table\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mq_table\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-73-6613428292fc>\u001b[0m in \u001b[0;36mplot_q_table\u001b[0;34m(q_table)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mplot_q_table\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq_table\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;34m\"\"\"Visualize max Q-value for each state and corresponding action.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mq_image\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq_table\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m       \u001b[0;31m# max Q-value for each state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mq_actions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq_table\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# best action for each state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/deep-learning/lib/python3.6/site-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36mamax\u001b[0;34m(a, axis, out, keepdims, initial)\u001b[0m\n\u001b[1;32m   2332\u001b[0m     \"\"\"\n\u001b[1;32m   2333\u001b[0m     return _wrapreduction(a, np.maximum, 'max', axis, None, out, keepdims=keepdims,\n\u001b[0;32m-> 2334\u001b[0;31m                           initial=initial)\n\u001b[0m\u001b[1;32m   2335\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2336\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/deep-learning/lib/python3.6/site-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36m_wrapreduction\u001b[0;34m(obj, ufunc, method, axis, dtype, out, **kwargs)\u001b[0m\n\u001b[1;32m     81\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpasskwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 83\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mufunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduce\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpasskwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAxisError\u001b[0m: axis 2 is out of bounds for array of dimension 1"
     ]
    }
   ],
   "source": [
    "def plot_q_table(q_table):\n",
    "    \"\"\"Visualize max Q-value for each state and corresponding action.\"\"\"\n",
    "    q_image = np.max(q_table, axis=2)       # max Q-value for each state\n",
    "    q_actions = np.argmax(q_table, axis=2)  # best action for each state\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(10, 10))\n",
    "    cax = ax.imshow(q_image, cmap='jet');\n",
    "    cbar = fig.colorbar(cax)\n",
    "    for x in range(q_image.shape[0]):\n",
    "        for y in range(q_image.shape[1]):\n",
    "            ax.text(x, y, q_actions[x, y], color='white',\n",
    "                    horizontalalignment='center', verticalalignment='center')\n",
    "    ax.grid(False)\n",
    "    ax.set_title(\"Q-table, size: {}\".format(q_table.shape))\n",
    "    ax.set_xlabel('position')\n",
    "    ax.set_ylabel('velocity')\n",
    "\n",
    "\n",
    "plot_q_table(agent.q_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:deep-learning]",
   "language": "python",
   "name": "conda-env-deep-learning-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
